# Chapter 18:
---
## **Introduction to Paging**:
Paging divides memory into fixed sized chucks.
Minimixes external fragmentation.

It is sometimes said that the operating system takes one of two approaches when solving most any space-management problem. ==The first approach is to chop things up into variable-sized pieces==, as we saw with ==segmentation== in virtual memory. Unfortunately, this solution has inherent difficulties. In particular, when dividing a space into different-size chunks, the space itself can become ==fragmented==, and thus allocation becomes more challenging over time.
Second Approach: 
- to chop up space into fixed-sized pieces.

In virtual memory, we call this idea ==paging==.

Instead of splitting up a process’s address space into some number of variable-sized logical segments (e.g., ==code, heap, stack==), we divide it into ==fixed-sized units, each of which we call a page==. Correspondingly, we view physical memory as an ==array of fixed-sized slots called page frames; each of these frames can contain a single virtual-memory page.==
![[Pasted image 20240220080005.png]]
## What’s Actually In The Page Table?
![[Pasted image 20240220080038.png]]
## **Page Table**: 
- To keep track of the mapping between virtual pages and physical page frames, the operating system maintains a ==data structure called a page table==. Each entry in the page table corresponds to a virtual page and contains information about its mapping to a physical page frame.

## **Address Translation**: 
- When a process accesses memory, the operating system translates the virtual address generated by the process into a physical address using the page table. This translation involves identifying the ==virtual page number (VPN)== and ==offset within the page== to determine the corresponding ==physical page frame number (PFN)==.
![[Pasted image 20240220080225.png]]
## **Page Table Structure**: 
- Page tables can vary in structure, but they typically ==contain several bits indicating the validity of a translation, protection attributes, and other control bits.== Page tables can be large, especially for systems with large address spaces, leading to memory overhead and potential performance issues.

## Page Table Entry (PTE):
- 4 bytes or 32 bits are used as a header to describe the page. 
- A list of page information is stored in the PTE.
![[Pasted image 20240220103859.png]]
- ==Swap Space== present bit p (figures out if space on secondary storage was used)
	- if p = 0, there is a trap (page fault)
1. goes to cache
	1. if miss goes to memory
2. goes to memory
	1. goes to TLB
		1. if miss goes to page table
	2. goes to page table
		1. if miss goes to secondary storage
3. goes to secondary storage
## **Memory Access with Paging**: 
- Paging introduces additional memory references for address translation, which can impact system performance. Each memory access may require accessing the page table, leading to increased memory access times.

## **Example and Memory Trace**: 
- The text provides an example of a simple memory access operation and traces the memory accesses involved in accessing memory using paging. It illustrates how paging impacts memory references and introduces complexity to memory management.

## **Challenges and Summary**: 
- While paging offers advantages such as efficient memory management and flexibility, it also poses challenges such as increased memory overhead and potential performance degradation due to additional memory accesses. Careful design and optimization are necessary to ensure efficient paging systems.

# Chapter 19 - HOW TO SPEED UP ADDRESS TRANSLATION:
---
==translation-lookaside buffer, or TLB== is part of the chip’s ==memory-management unit (MMU)==, and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an ==address-translation cache.==

the hardware ==first checks the TLB to see if the desired translation is held therein==; if so, the ==translation is performed (quickly) without having to consult the page table== (which has all translations). Because of their tremendous performance impact, TLBs in a real sense make virtual memory possible
![[Pasted image 20240220104434.png]]

It begins by extracting the virtual page number (VPN) from the virtual address and checking if the TLB contains the translation for this VPN. 
	If found (a ==TLB hit)==, the ==physical address (PA) is formed using the page frame number (PFN) from the TLB entry and the offset from the original virtual address.==
	Memory access is then performed, assuming protection checks pass.
	==If the translation is not found in the TLB (a TLB miss)==, the hardware accesses the page table to find the translation and updates the TLB with the translation if it is valid and accessible. Once the TLB is updated, the instruction is retried, resulting in a TLB hit and faster memory access.
![[Pasted image 20240220082824.png]]

The importance of TLBs in ==improving performance by caching translations==

## How caching is useful:
The idea behind hardware caches is to ==take advantage of locality== in instruction and data references. There are usually two types of locality: ==temporal locality== and ==spatial locality==. With ==temporal locality, the idea is that an instruction or data item that has been recently accessed will likely be re-accessed soon in the future.== Think of loop variables or instructions in a loop. With ==spatial locality, the idea is that if a program accesses memory at address x, it will likely soon access memory near x==.

## Problems with TLB caching in certine Programs:
if the ==number of pages a program accesses in a short period of time exceeds the number of pages that fit into the TLB==, the program will generate a large number of ==TLB misses==, and thus run quite a bit more slowly. We refer to this phenomenon as exceeding the ==TLB coverage==, and it can be quite a problem for certain programs.


